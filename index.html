<!DOCTYPE html>
<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Liang Zhang</title>
    <link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
    <header>
        <h1>Liang Zhang</h1>
        <p>Email: (first name).(last name)@inf.ethz.ch</p>
    </header>

    <section id="biography">
        <p>
            I am a second-year Ph.D. student in computer science at the <a href="https://learning-systems.org/">Max Planck ETH Center for Learning Systems</a>, jointly advised by <a href="https://odi.inf.ethz.ch/niaohe">Niao He</a> from ETH Zurich and <a href="https://sites.google.com/view/mmuehlebach/">Michael Muehlebach</a> from the Max Planck Institute for Intelligent Systems.
        </p>
        <p>
            My research focus lies in the intersection between optimization and machine learning. I have a keen interest in developing optimization algorithms for trustworthy and reliable machine learning, particularly in areas such as differential privacy, fairness, and reproducibility.
        </p>
        <p>
            Prior to this, I received my master’s degree in computer science from ETH Zurich in 2021 and bachelor’s degree in physics from Peking University in 2019.
        </p>
    </section>

    <section id="publications">
        <h2>Publications</h2>
        <p><a href="https://scholar.google.com/citations?user=OIgmMCkAAAAJ&hl=en">Google Scholar</a></p>
        <ul>
            <li>
                <b>Liang Zhang</b>*, Junchi Yang*, Amin Karbasi, Niao He. Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization.
                <b>NeurIPS 2023 (Spotlight)</b>. <a href="https://arxiv.org/abs/2310.17759">[arXiv]</a>
            </li>
            <li>
                <b>Liang Zhang</b>, Kiran Koshy Thekumparampil, Sewoong Oh, Niao He. DPZero: Dimension-Independent and Differentially Private Zeroth-Order Optimization.
                <b>NeurIPS 2023 Workshop on Federated Learning in the Age of Foundation Models</b>. (Working Paper) <a href="https://arxiv.org/abs/2310.09639">[arXiv]</a>
            </li>
            <li>
                <b>Liang Zhang</b>, Kiran Koshy Thekumparampil, Sewoong Oh, Niao He. Bring Your Own Algorithm for Optimal Differentially Private Stochastic Minimax Optimization.
                <b>NeurIPS 2022</b>. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/e46fc33e80e9fa2febcdb058fba4beca-Abstract-Conference.html">[NeurIPS]</a> <a href="https://arxiv.org/abs/2206.00363">[arXiv]</a>
            </li>
            <li>
                Siqi Zhang*, Yifan Hu*, <b>Liang Zhang</b>, Niao He. Generalization Bounds of Nonconvex-(Strongly)-Concave Stochastic Minimax Optimization.
                (Working Paper) <a href="https://arxiv.org/abs/2205.14278">[arXiv]</a>
            </li>
        </ul>
    </section>

    <section id="talks">
        <h2>Talks</h2>
        <ul>
            <li><a href="https://www.siam.org/conferences/cm/conference/op23">SIAM Conference on Optimization (OP23)</a>, on differentially private stochastic minimax optimization. </li>
        </ul>
    </section>

    <section id="teaching">
        <h2>Teaching</h2>
        <ul>
            <li><a href="https://odi.inf.ethz.ch/teaching/FoRL.html">Foundations of Reinforcement Learning</a>, ETH Zurich. </li>
        </ul>
    </section>

    <section id="service">
        <h2>Service</h2>
        <ul>
            <li>Reviewer for NeurIPS, ICLR, and ICML. </li>
        </ul>
    </section>

    <footer>
        <p>&copy; Generated by ChatGPT. Style follows from <a href="https://www.mit.edu/~wsshin/jemdoc+mathjax.html">jemdoc+MathJax</a>. Last update: Jan, 2024.</p>
    </footer>
</body>
</html>