<!DOCTYPE html>
<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Liang Zhang</title>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
    <link rel="manifest" href="images/site.webmanifest">
</head>
<body>
    <header>
        <h1>Liang Zhang</h1>
        <!-- <img src="images/android-chrome-512x512.png" class="profile-pic"> -->
    </header>

    <nav>
        <a href="#about" class="nav-link">About</a>
        <a href="#publications" class="nav-link">Publications</a>
        <a href="#teaching" class="nav-link">Teaching</a>
        <a href="" onclick="alert('(first name).(last name)@inf.ethz.ch')">Email</a>
        <a href="https://scholar.google.com/citations?user=OIgmMCkAAAAJ&hl=en">Google Scholar</a>
    </nav>

    <section id="about">
        <h2>About</h2>
        <p>
            I am a 4th-year Ph.D. student in computer science at the <a href="https://learning-systems.org/">Max Planck ETH Center for Learning Systems</a>, jointly advised by <a href="https://odi.inf.ethz.ch/niaohe">Niao He</a> from ETH Zurich and <a href="https://sites.google.com/view/mmuehlebach/">Michael Muehlebach</a> from the Max Planck Institute for Intelligent Systems. I also closely collaborate with <a href="https://homes.cs.washington.edu/~sewoong/">Sewoong Oh</a> from University of Washington and <a href="https://scholar.google.com/citations?user=0gJQCIgAAAAJ&hl=en">Kiran Koshy Thekumparampil</a> from Amazon. 
        </p>
        <p>
            Prior to this, I received my master's degree in computer science from <a href="https://ethz.ch/en.html">ETH Zurich</a> in 2021 and bachelor's degree in physics from <a href="https://english.pku.edu.cn/">Peking University</a> in 2019.
        </p>
        <p>
            My research focuses on the intersection between optimization and machine learning, and I have a keen interest in developing efficient optimization algorithms for trustworthy machine learning. My current research targets the <b>efficient fine-tuning of large language models (LLMs)</b>, particularly on:
        </p>
        <ul>
            <li>
                <b>Zeroth-order optimization</b> for memory-efficient LLMs fine-tuning;
                see DPZero <a href="https://proceedings.mlr.press/v235/zhang24af.html">[ICML'24]</a> on private and memory-efficient LLMs fine-tuning,
                and <a href="https://arxiv.org/abs/2506.05454">[NeurIPS'25]</a> on implicit regularization of zeroth-order optimization towards flat minima.
            </li>
            <li>
                Parameter-efficient fine-tuning (PEFT) such as <b>LoRA</b>;
                see <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/4eb2c0adafbe71269f3a772c130f9e53-Abstract-Conference.html">[NeurIPS'24]</a> on balancedness of LoRA matrix,
                NoRA <a href="https://openreview.net/forum?id=YTEwJaBdh0">[ICLR'25]</a> on LoRA initialization,
                and PoLAR <a href="https://arxiv.org/abs/2506.03133">[NeurIPS'25]</a> on how rank inefficiency in LoRA can be resolved with PoLAR.
            </li>
        </ul>
    </section>

    <section id="publications">
        <h2>Publications</h2>

        <h3>Preprint</h3>
        <ul>
            <li>
                A Hessian-Aware Stochastic Differential Equation for Modelling SGD. <br>
                Xiang Li, Zebang Shen, <b>Liang Zhang</b>, Niao He. <br>
                Working Paper <a href="https://arxiv.org/abs/2405.18373">[arXiv]</a> <a href="https://openreview.net/forum?id=GH5XHcHTS5">[HiLD@ICML'24]</a>
            </li>
        </ul>

        <h3>Journal Article</h3>
        <ul>
            <li>
                Primal Methods for Variational Inequality Problems with Functional Constraints. <br>
                <b>Liang Zhang</b>, Niao He, Michael Muehlebach. <br>
                <b>Math. Program., 2025</b> <a href="https://doi.org/10.1007/s10107-025-02206-3">[Springer]</a> <a href="https://arxiv.org/abs/2403.12859">[arXiv]</a>
            </li>
        </ul>

        <h3>Conference Paper</h3>
        <ul>
            <li>
                Zeroth-Order Optimization Finds Flat Minima. <br>
                <b>Liang Zhang</b>, Bingcong Li, Kiran Koshy Thekumparampil, Sewoong Oh, Michael Muehlebach, Niao He. <br>
                <b>NeurIPS, 2025</b> <a href="https://arxiv.org/abs/2506.05454">[arXiv]</a>
            </li>
            <li>
                PoLAR: Polar-Decomposed Low-Rank Adapter Representation. <br>
                Kai Lion, <b>Liang Zhang</b>, Bingcong Li, Niao He. <br>
                <b>NeurIPS, 2025</b> <a href="https://arxiv.org/abs/2506.03133">[arXiv]</a> <a href="https://github.com/kcc-lion/polar">[GitHub]</a>
            </li>
            <li>
                On the Crucial Role of Initialization for Matrix Factorization. <br>
                Bingcong Li, <b>Liang Zhang</b>, Aryan Mokhtari, Niao He. <br>
                <b>ICLR, 2025</b> <a href="https://openreview.net/forum?id=YTEwJaBdh0">[OpenReview]</a> <a href="https://arxiv.org/abs/2410.18965">[arXiv]</a> <a href="https://openreview.net/forum?id=WE5B0N2Jr0">[OPT-ML@NeurIPS'24]</a> <a href="https://openreview.net/forum?id=tCrEQpnilb">[CPAL'25]</a>
            </li>
            <li>
                Implicit Regularization of Sharpness-Aware Minimization for Scale-Invariant Problems. <br>
                Bingcong Li, <b>Liang Zhang</b>, Niao He. <br>
                <b>NeurIPS, 2024</b> <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/4eb2c0adafbe71269f3a772c130f9e53-Abstract-Conference.html">[NeurIPS]</a> <a href="https://openreview.net/forum?id=oSOVME9kl2">[OpenReview]</a> <a href="https://arxiv.org/abs/2410.14802">[arXiv]</a> <a href="https://openreview.net/forum?id=chI7jvNkwK">[TF2M@ICML'24]</a>
            </li>
            <li>
                DPZero: Private Fine-Tuning of Language Models without Backpropagation. <br>
                <b>Liang Zhang</b>, Bingcong Li, Kiran Koshy Thekumparampil, Sewoong Oh, Niao He. <br>
                <b>ICML, 2024</b> <a href="https://proceedings.mlr.press/v235/zhang24af.html">[ICML]</a> <a href="https://openreview.net/forum?id=QJkG8Mln72">[OpenReview]</a> <a href="https://arxiv.org/abs/2310.09639">[arXiv]</a> <a href="https://github.com/Liang137/DPZero">[GitHub]</a> <a href="https://youtu.be/wdjloGaAOPs">[Video]</a> <a href="https://openreview.net/forum?id=s7hquGszME">[FL-FM@NeurIPS'23]</a> <a href="https://responsiblecomputing.org/forc-2024-accepted-papers/">[FORC'24]</a> <a href="https://tpdp.journalprivacyconfidentiality.org/2024/">[TPDP'24]</a>
            </li>
            <li>
                Generalization Bounds of Nonconvex-(Strongly)-Concave Stochastic Minimax Optimization. <br>
                Siqi Zhang*, Yifan Hu*, <b>Liang Zhang</b>, Niao He. <br>
                <b>AISTATS, 2024</b> <a href="https://proceedings.mlr.press/v238/zhang24c.html">[AISTATS]</a> <a href="https://arxiv.org/abs/2205.14278">[arXiv]</a> <a href="https://openreview.net/forum?id=_AHlSVY2ebg">[OPT-ML@NeurIPS'22]</a>
            </li>
            <li>
                Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization. <br>
                <b>Liang Zhang</b>*, Junchi Yang*, Amin Karbasi, Niao He. <br>
                <b>NeurIPS, 2023 (Spotlight)</b> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/38c5feed4b72c96f6cf925ccc9832ecf-Abstract-Conference.html">[NeurIPS]</a> <a href="https://openreview.net/forum?id=hCdqDkA25J">[OpenReview]</a> <a href="https://arxiv.org/abs/2310.17759">[arXiv]</a>
            </li>
            <li>
                Bring Your Own Algorithm for Optimal Differentially Private Stochastic Minimax Optimization. <br>
                <b>Liang Zhang</b>, Kiran Koshy Thekumparampil, Sewoong Oh, Niao He. <br>
                <b>NeurIPS, 2022</b> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/e46fc33e80e9fa2febcdb058fba4beca-Abstract-Conference.html">[NeurIPS]</a> <a href="https://openreview.net/forum?id=fRbvozXEGTb">[OpenReview]</a> <a href="https://arxiv.org/abs/2206.00363">[arXiv]</a>
            </li>
        </ul>
    </section>

    <section id="thesis">
        <h2>Thesis</h2>
        <ul>
            <li>
                Variance Reduction for Non-Convex Stochastic Optimization: General Analysis and New Applications. <br>
                Master Thesis <a href="https://doi.org/10.3929/ethz-b-000507454">[ETH]</a>. Supervised by <a href="https://sites.google.com/view/yifan-hu/home">Yifan Hu</a> and Niao He.
            </li>
        </ul>
    </section>

    <section id="talks">
        <h2>Presentations</h2>
        <ul>
            <!-- <li>ICLR 2025, Singapore. Poster on NoRA. </li> -->
            <li><a href="https://cpal.cc">Conference on Parsimony and Learning (CPAL) 2025</a>, Stanford. Poster presentation on LoRA initialization (NoRA). </li>
            <!-- <li>NeurIPS 2024, Vancouver. Poster on BAR and NoRA. </li> -->
            <li><a href="https://tpdp.journalprivacyconfidentiality.org/2024/">Theory and Practice of Differential Privacy (TPDP) 2024</a>, Boston. Poster presentation on DPZero. </li>
            <!-- <li>ICML 2024, Vienna. Poster on DPZero, BAR, and HA-SME. </li> -->
            <!-- <li>NeurIPS 2023, New Orleans. Poster on reproducibility and DPZero. </li> -->
            <li><a href="https://www.siam.org/conferences-events/past-event-archive/op23/">SIAM Conference on Optimization (OP23)</a>, Seattle. Talk on differentially private stochastic minimax optimization. </li>
        </ul>
    </section>

    <section id="teaching">
        <h2>Teaching Assistant</h2>
        <ul>
            <li><a href="https://odi.inf.ethz.ch/teaching/ODS24.html">Optimization for Data Science</a> (Spring 2024, Autumn 2025),  ETH Zurich. </li>
            <li><a href="https://odi.inf.ethz.ch/teaching/FoRL.html">Foundations of Reinforcement Learning</a> (Autumn 2021, Spring 2023), ETH Zurich. </li>
            <li><a href="https://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2025S&ansicht=ALLE&lerneinheitId=187220&lang=en">Large-Scale Convex Optimization</a> (Spring 2025), ETH Zurich. </li>
            <li><a href="https://systems.ethz.ch/education/courses/2024-autumn-semester/big-data.html">Big Data</a> (Autumn 2024), ETH Zurich. </li>
        </ul>
    </section>

    <section id="supervision">
        <h2>Project Supervision</h2>
        <ul>
            <li>
                Colomban Duclaux, semester project at ETH Zurich, 2025 (co-supervised with <a href="https://bingcongli.github.io/">Bingcong Li</a>). <br>
                Topic: Aligning LLMs with Differentially Private Reinforcement Learning from Human Feedback.
            </li>
            <li>
                Yudong Wei, semester project at ETH Zurich, 2025 (co-supervised with Bingcong Li). <br>
                Topic: Overparameterized Matrix Sensing.
                <!-- Topic: Riemannian Optimization for Overparameterized Matrix Sensing. -->
            </li>
            <li>
                Lejs Behric, bachelor thesis at ETH Zurich, 2024 (co-supervised with Bingcong Li and Kiran Koshy Thekumparampil). <br>
                Topic: Fine-tuning LLMs with Efficient Zeroth-Order Optimization.
            </li>
            <li>
                Nikolija Bojkovic, <a href="https://inf.ethz.ch/studies/summer-research-fellowship.html">Student Summer Research Fellowship</a> at ETH Zurich, 2024 (co-supervised with Bingcong Li). <br>
                Topic: Differentially Private Stochastic Zeroth-Order Optimization.
            </li>
            <li>
                Lukas Altun, bachelor thesis at ETH Zurich, 2024 (co-supervised with Bingcong Li). <br>
                Topic: Computationally Efficient Sharpness-Aware Minimization.
            </li>
            <li>
                Martin Mason, bachelor thesis at ETH Zurich, 2023. <br>
                Topic: Exploring Gradient Clipping in Private Machine Learning.
            </li>
        </ul>
    </section>

    <section id="service">
        <h2>Service</h2>
        <ul>
            <li>Reviewers for JMLR, TMLR, NeurIPS (2023, 2024, 2025), ICLR (2023, 2024, 2025, 2026), ICML (2024, 2025), and AISTATS (2025, 2026). </li>
            <li>Top reviewers for NeurIPS (2024). </li>
        </ul>
    </section>

    <footer>
        <p>&copy; Template generated by ChatGPT &nbsp; | &nbsp; Last update: Sep, 2025</p>
    </footer>
</body>
</html>