<!DOCTYPE html>
<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Liang Zhang</title>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
    <link rel="manifest" href="images/site.webmanifest">
</head>
<body>
    <header>
        <h1>Liang Zhang</h1>
        <p>Email: (first name).(last name)@inf.ethz.ch</p>
    </header>

    <section id="about">
        <h2>About</h2>
        <p>
            I am a third-year Ph.D. student in computer science at the <a href="https://learning-systems.org/">Max Planck ETH Center for Learning Systems</a>, jointly advised by <a href="https://odi.inf.ethz.ch/niaohe">Niao He</a> from ETH Zurich and <a href="https://sites.google.com/view/mmuehlebach/">Michael Muehlebach</a> from the Max Planck Institute for Intelligent Systems.
        </p>
        <p>
            My research focus lies in the intersection between optimization and machine learning. I have a keen interest in developing optimization algorithms for trustworthy and reliable machine learning, particularly in areas such as differential privacy and reproducibility.
        </p>
        <p>
            Prior to this, I received my master's degree in computer science from <a href="https://ethz.ch/en.html">ETH Zurich</a> in 2021 and bachelor's degree in physics from <a href="https://english.pku.edu.cn/">Peking University</a> in 2019.
        </p>
    </section>

    <section id="publications">
        <h2>Publications</h2>
        <p><a href="https://scholar.google.com/citations?user=OIgmMCkAAAAJ&hl=en">Google Scholar</a></p>
        <ul>
            <li>
                On the Crucial Role of Initialization for Matrix Factorization. <br>
                Bingcong Li, <b>Liang Zhang</b>, Aryan Mokhtari, Niao He. <br>
                Working Paper <a href="https://arxiv.org/abs/2410.18965">[arXiv]</a>
            </li>
            <li>
                A Hessian-Aware Stochastic Differential Equation for Modelling SGD. <br>
                Xiang Li, Zebang Shen, <b>Liang Zhang</b>, Niao He. <br>
                Working Paper <a href="https://arxiv.org/abs/2405.18373">[arXiv]</a> <a href="https://openreview.net/forum?id=GH5XHcHTS5">[Workshop]</a>
            </li>
            <li>
                Primal Methods for Variational Inequality Problems with Functional Constraints. <br>
                <b>Liang Zhang</b>, Niao He, Michael Muehlebach. <br>
                Working Paper <a href="https://arxiv.org/abs/2403.12859">[arXiv]</a>
            </li>
            <li>
                Implicit Regularization of Sharpness-Aware Minimization for Scale-Invariant Problems. <br>
                Bingcong Li, <b>Liang Zhang</b>, Niao He. <br>
                <b>NeurIPS 2024</b> <a href="https://arxiv.org/abs/2410.14802">[arXiv]</a> <a href="https://openreview.net/forum?id=chI7jvNkwK">[Workshop]</a>
            </li>
            <li>
                DPZero: Private Fine-Tuning of Language Models without Backpropagation. <br>
                <b>Liang Zhang</b>, Bingcong Li, Kiran Koshy Thekumparampil, Sewoong Oh, Niao He. <br>
                <b>ICML 2024</b> <a href="https://proceedings.mlr.press/v235/zhang24af.html">[ICML]</a> <a href="https://openreview.net/forum?id=QJkG8Mln72">[OpenReview]</a> <a href="https://arxiv.org/abs/2310.09639">[arXiv]</a> <a href="https://github.com/Liang137/DPZero">[GitHub]</a> <a href="https://youtu.be/wdjloGaAOPs">[Video]</a> <a href="https://openreview.net/forum?id=s7hquGszME">[Workshop]</a>
            </li>
            <li>
                Generalization Bounds of Nonconvex-(Strongly)-Concave Stochastic Minimax Optimization. <br>
                Siqi Zhang*, Yifan Hu*, <b>Liang Zhang</b>, Niao He. <br>
                <b>AISTATS 2024</b> <a href="https://proceedings.mlr.press/v238/zhang24c.html">[AISTATS]</a> <a href="https://arxiv.org/abs/2205.14278">[arXiv]</a> <a href="https://openreview.net/forum?id=_AHlSVY2ebg">[Workshop]</a>
            </li>
            <li>
                Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization. <br>
                <b>Liang Zhang</b>*, Junchi Yang*, Amin Karbasi, Niao He. <br>
                <b>NeurIPS 2023 (Spotlight)</b> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/38c5feed4b72c96f6cf925ccc9832ecf-Abstract-Conference.html">[NeurIPS]</a> <a href="https://openreview.net/forum?id=hCdqDkA25J">[OpenReview]</a> <a href="https://arxiv.org/abs/2310.17759">[arXiv]</a>
            </li>
            <li>
                Bring Your Own Algorithm for Optimal Differentially Private Stochastic Minimax Optimization. <br>
                <b>Liang Zhang</b>, Kiran Koshy Thekumparampil, Sewoong Oh, Niao He. <br>
                <b>NeurIPS 2022</b> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/e46fc33e80e9fa2febcdb058fba4beca-Abstract-Conference.html">[NeurIPS]</a> <a href="https://openreview.net/forum?id=fRbvozXEGTb">[OpenReview]</a> <a href="https://arxiv.org/abs/2206.00363">[arXiv]</a>
            </li>
        </ul>
    </section>

    <section id="talks">
        <h2>Talks</h2>
        <ul>
            <li><a href="https://www.siam.org/conferences/cm/conference/op23">SIAM Conference on Optimization (OP23)</a>, on differentially private stochastic minimax optimization. </li>
        </ul>
    </section>

    <section id="teaching">
        <h2>Teaching</h2>
        <ul>
            <li><a href="https://odi.inf.ethz.ch/teaching/ODS24.html">Optimization for Data Science</a> (Spring 2024),  ETH Zurich. </li>
            <li><a href="https://odi.inf.ethz.ch/teaching/FoRL.html">Foundations of Reinforcement Learning</a> (Autumn 2021, Spring 2023), ETH Zurich. </li>
            <li><a href="https://systems.ethz.ch/education/courses/2024-autumn-semester/big-data.html">Big Data</a> (Autumn 2024), ETH Zurich. </li>
        </ul>
    </section>

    <section id="service">
        <h2>Service</h2>
        <ul>
            <li>Reviewer for NeurIPS (2023, 2024), ICLR (2023, 2024, 2025), ICML (2024), and AISTATS (2025). </li>
        </ul>
    </section>

    <footer>
        <p>&copy; Generated by ChatGPT. Style follows from <a href="https://www.mit.edu/~wsshin/jemdoc+mathjax.html">jemdoc+MathJax</a>. Last update: Oct, 2024.</p>
    </footer>
</body>
</html>